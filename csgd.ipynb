{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演示一下 k-mean 的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape is:  (16, 3, 24, 24)\n",
      "input data shape is:  (16, 1728)\n",
      "number of clusters is:  4\n",
      "result of km:  [0 0 3 0 2 0 2 1 0 1 0 2 2 0 0 2]\n",
      "cluster result:  [[0, 1, 3, 5, 8, 10, 13, 14], [7, 9], [4, 6, 11, 12, 15], [2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "data = np.random.rand(16,3,24,24)\n",
    "print(\"data shape is: \", data.shape)\n",
    "data_input = np.reshape(data, (data.shape[0], -1))\n",
    "print(\"input data shape is: \", data_input.shape)\n",
    "\n",
    "n_clusters = 4\n",
    "print(\"number of clusters is: \", n_clusters)\n",
    "\n",
    "km = KMeans(n_clusters=n_clusters)\n",
    "km.fit(data_input)\n",
    "print(\"result of km: \", km.labels_)\n",
    "\n",
    "result = [[] for i in range(n_clusters)]\n",
    "for i, c in enumerate(km.labels_):\n",
    "        result[c].append(i)\n",
    "print(\"cluster result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以 vgg16 举例子。\n",
    "## 获取卷积的参数，存放字典容器中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from collections import namedtuple\n",
    "\n",
    "NamedParamValue = namedtuple('NamedParamValue', ['name', 'value'])\n",
    "model = torchvision.models.vgg16(pretrained = True)\n",
    "\n",
    "# summary(model.cuda(), input_size=(3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key & value shape:  features.0.weight   torch.Size([64, 3, 3, 3])\n",
      "key & value shape:  features.2.weight   torch.Size([64, 64, 3, 3])\n",
      "key & value shape:  features.5.weight   torch.Size([128, 64, 3, 3])\n",
      "key & value shape:  features.7.weight   torch.Size([128, 128, 3, 3])\n",
      "key & value shape:  features.10.weight   torch.Size([256, 128, 3, 3])\n",
      "key & value shape:  features.12.weight   torch.Size([256, 256, 3, 3])\n",
      "key & value shape:  features.14.weight   torch.Size([256, 256, 3, 3])\n",
      "key & value shape:  features.17.weight   torch.Size([512, 256, 3, 3])\n",
      "key & value shape:  features.19.weight   torch.Size([512, 512, 3, 3])\n",
      "key & value shape:  features.21.weight   torch.Size([512, 512, 3, 3])\n",
      "key & value shape:  features.24.weight   torch.Size([512, 512, 3, 3])\n",
      "key & value shape:  features.26.weight   torch.Size([512, 512, 3, 3])\n",
      "key & value shape:  features.28.weight   torch.Size([512, 512, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "def get_all_conv_kernel_namedvalue_as_list():\n",
    "    result = []\n",
    "\n",
    "    for k, v in model.state_dict().items():\n",
    "        if v.dim() == 4:\n",
    "            print(\"key & value shape: \", k, \" \", v.shape)\n",
    "            result.append(NamedParamValue(name=k, value=v.cpu().numpy()))\n",
    "            \n",
    "    return result\n",
    "\n",
    "kernel_namedvalue_list = get_all_conv_kernel_namedvalue_as_list() \n",
    "# print(kernel_namedvalue_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取目标通道列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin channels: \n",
      " [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512] \n",
      "\n",
      "target channels: \n",
      " [40, 40, 80, 80, 160, 160, 160, 320, 320, 320, 320, 320, 320] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from constants import *\n",
    "\n",
    "# 原始通道数列表\n",
    "VGG_ORIGIN_DEPS_FLATTENED = [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]\n",
    "deps = VGG_ORIGIN_DEPS_FLATTENED\n",
    "\n",
    "target_deps = [[d * 13 // 16 for d in deps],\n",
    "                [d * 11 // 16 for d in deps],\n",
    "                [d * 5 // 8 for d in deps]]\n",
    "\n",
    "target_deps_1 = [48, 48, 64, 64, 128, 128, 128, 256, 256, 256, 256, 256, 256, 256]\n",
    "\n",
    "print(\"origin channels: \\n\", deps, '\\n')\n",
    "print(\"target channels: \\n\", list(target_deps[2]), '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用 kernel_namedvalue_list 和 target_deps，进行聚类操作，生成每层聚类后的通道情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 64 filters into 40 clusters\n",
      "cluster 64 filters into 40 clusters\n",
      "cluster 128 filters into 80 clusters\n",
      "cluster 128 filters into 80 clusters\n",
      "cluster 256 filters into 160 clusters\n",
      "cluster 256 filters into 160 clusters\n",
      "cluster 256 filters into 160 clusters\n",
      "cluster 512 filters into 320 clusters\n",
      "cluster 512 filters into 320 clusters\n",
      "cluster 512 filters into 320 clusters\n",
      "cluster 512 filters into 320 clusters\n",
      "cluster 512 filters into 320 clusters\n",
      "cluster 512 filters into 320 clusters\n"
     ]
    }
   ],
   "source": [
    "def _is_follower(layer_idx, pacesetter_dict):\n",
    "    followers_and_pacesetters = set(pacesetter_dict.keys())\n",
    "    return (layer_idx in followers_and_pacesetters) and (pacesetter_dict[layer_idx] != layer_idx)\n",
    "\n",
    "def cluster_by_kmeans(kernel_value, num_cluster):\n",
    "    assert kernel_value.ndim == 4  # n,c,h,w\n",
    "    x = np.reshape(kernel_value, (kernel_value.shape[0], -1))  # n, c*h*w\n",
    "    if num_cluster == x.shape[0]:  # if num_cluster == n, result = [0, 1, ..., n]\n",
    "        result = [[i] for i in range(num_cluster)]\n",
    "        return result\n",
    "    else:\n",
    "        print('cluster {} filters into {} clusters'.format(x.shape[0], num_cluster))\n",
    "    km = KMeans(n_clusters=num_cluster)  # use sklearn.cluster.KMeans to cluster kernel_value\n",
    "    km.fit(x)\n",
    "    result = []  # record result\n",
    "    for j in range(num_cluster):\n",
    "        result.append([])\n",
    "    for i, c in enumerate(km.labels_):\n",
    "        result[c].append(i)\n",
    "    for r in result:\n",
    "        assert len(r) > 0\n",
    "    return result\n",
    "\n",
    "def get_layer_idx_to_clusters(kernel_namedvalue_list, target_deps, pacesetter_dict):\n",
    "    result = {}\n",
    "    for layer_idx, named_kv in enumerate(kernel_namedvalue_list):\n",
    "        num_filters = named_kv.value.shape[0]\n",
    "        if pacesetter_dict is not None and _is_follower(layer_idx, pacesetter_dict):\n",
    "            continue\n",
    "        if num_filters > target_deps[layer_idx]:\n",
    "            result[layer_idx] = cluster_by_kmeans(kernel_value=named_kv.value, num_cluster=target_deps[layer_idx])\n",
    "        elif num_filters < target_deps[layer_idx]:\n",
    "            print(num_filters, target_deps[layer_idx])\n",
    "            raise ValueError('wrong target dep')\n",
    "    return result\n",
    "\n",
    "layer_idx_to_clusters = get_layer_idx_to_clusters(kernel_namedvalue_list=kernel_namedvalue_list,\n",
    "                                                                  target_deps=target_deps[2],\n",
    "                                                                  pacesetter_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11],\n",
       " [33],\n",
       " [12, 37, 44],\n",
       " [3, 26, 27, 43],\n",
       " [10, 17],\n",
       " [34],\n",
       " [53],\n",
       " [25],\n",
       " [45, 47, 61],\n",
       " [7, 52],\n",
       " [0],\n",
       " [58],\n",
       " [9],\n",
       " [36],\n",
       " [8],\n",
       " [51],\n",
       " [40, 46],\n",
       " [2],\n",
       " [55],\n",
       " [42],\n",
       " [59],\n",
       " [39],\n",
       " [19],\n",
       " [57],\n",
       " [63],\n",
       " [1],\n",
       " [32],\n",
       " [29, 56],\n",
       " [14],\n",
       " [20],\n",
       " [5, 13, 15, 24, 30, 41, 50, 54, 62],\n",
       " [21],\n",
       " [35, 60],\n",
       " [23],\n",
       " [6],\n",
       " [38, 48],\n",
       " [16, 28],\n",
       " [18],\n",
       " [4],\n",
       " [22, 31, 49]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_idx_to_clusters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过上面这几步骤，我们得到了每层通道聚类后的结果。\n",
    "## 接下来要利用这结果更新权重。\n",
    "\n",
    "利用 \n",
    "1. 模型原本每层的卷积数：deps\n",
    "2. 每层聚类的结果：layer_idx_to_clusters\n",
    "3. 模型原本每层的卷积参数：kernel_namedvalue_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_merge_matrix_for_kernel(deps, layer_idx_to_clusters, kernel_namedvalue_list):\n",
    "    result = {}\n",
    "    for layer_idx, clusters in layer_idx_to_clusters.items():\n",
    "        # 每层的通道数目\n",
    "        num_filters = deps[layer_idx]\n",
    "        # 构建 num_filters * num_filters 0的矩阵\n",
    "        merge_trans_mat = np.zeros((num_filters, num_filters), dtype=np.float32)\n",
    "        # 距离 clusters, 16 聚类 4 的结果 [[1, 10, 11, 12, 14], [3, 6], [0, 4, 7, 8, 9, 13], [2, 5, 15]]\n",
    "        for clst in clusters:\n",
    "            # 此时 clst 分别是 [1, 10, 11, 12, 14], [3, 6], [0, 4, 7, 8, 9, 13], [2, 5, 15]\n",
    "            if len(clst) == 1:\n",
    "                merge_trans_mat[clst[0], clst[0]] = 1\n",
    "                continue\n",
    "            sc = sorted(clst)       # Ideally, clst should have already been sorted in ascending order\n",
    "            for ei in sc:\n",
    "                for ej in sc:\n",
    "                    merge_trans_mat[ei, ej] = 1 / len(clst)\n",
    "        result[kernel_namedvalue_list[layer_idx].name] = torch.from_numpy(merge_trans_mat).cuda()\n",
    "        # 这样每层都能得到一个 聚类后id 的 matrix\n",
    "        # 这个 matrix 是为了加快计算用的\n",
    "    return result\n",
    "\n",
    "deps = [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]\n",
    "\n",
    "param_name_to_merge_matrix = generate_merge_matrix_for_kernel(deps, layer_idx_to_clusters, kernel_namedvalue_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.3333, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1111, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_name_to_merge_matrix['features.0.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上面获得了每层卷积权重的一个 matrix，然后将这个 matrix 复制三份，给卷积 bias，bn 的 weight, bn 的 bias。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_KEYWORD = 'weight'\n",
    "\n",
    "def add_vecs_to_merge_mat_dicts(param_name_to_merge_matrix):\n",
    "    kernel_names = set(param_name_to_merge_matrix.keys())\n",
    "    for name in kernel_names:\n",
    "        bias_name = name.replace(KERNEL_KEYWORD, 'conv.bias')\n",
    "        gamma_name = name.replace(KERNEL_KEYWORD, 'bn.weight')\n",
    "        beta_name = name.replace(KERNEL_KEYWORD, 'bn.bias')\n",
    "        param_name_to_merge_matrix[bias_name] = param_name_to_merge_matrix[name]\n",
    "        param_name_to_merge_matrix[gamma_name] = param_name_to_merge_matrix[name]\n",
    "        param_name_to_merge_matrix[beta_name] = param_name_to_merge_matrix[name]\n",
    "\n",
    "add_vecs_to_merge_mat_dicts(param_name_to_merge_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(param_name_to_merge_matrix['features.0.weight'].shape)\n",
    "print(param_name_to_merge_matrix['features.0.conv.bias'].shape)\n",
    "print(param_name_to_merge_matrix['features.0.bn.weight'].shape)\n",
    "print(param_name_to_merge_matrix['features.0.bn.bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来是生成权重衰减的 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-4\n",
    "weight_decay_bias = 0\n",
    "centri_strength = 3e-3\n",
    "\n",
    "def generate_decay_matrix_for_kernel_and_vecs(deps, layer_idx_to_clusters, kernel_namedvalue_list,\n",
    "                                              weight_decay, weight_decay_bias, centri_strength):\n",
    "    result = {}\n",
    "    #   for the kernel\n",
    "    for layer_idx, clusters in layer_idx_to_clusters.items():\n",
    "        num_filters = deps[layer_idx]\n",
    "        decay_trans_mat = np.zeros((num_filters, num_filters), dtype=np.float32)\n",
    "        for clst in clusters:\n",
    "            sc = sorted(clst)\n",
    "            for ee in sc:\n",
    "                decay_trans_mat[ee, ee] = weight_decay + centri_strength\n",
    "                for p in sc:\n",
    "                    decay_trans_mat[ee, p] += -centri_strength / len(clst)\n",
    "        kernel_mat = torch.from_numpy(decay_trans_mat).cuda()\n",
    "        result[kernel_namedvalue_list[layer_idx].name] = kernel_mat\n",
    "\n",
    "    #   for the vec params (bias, beta and gamma), we use 0.1 * centripetal strength\n",
    "    for layer_idx, clusters in layer_idx_to_clusters.items():\n",
    "        num_filters = deps[layer_idx]\n",
    "        decay_trans_mat = np.zeros((num_filters, num_filters), dtype=np.float32)\n",
    "        for clst in clusters:\n",
    "            sc = sorted(clst)\n",
    "            for ee in sc:\n",
    "                # Note: using smaller centripetal strength on the scaling factor of BN improve the performance in some of the cases\n",
    "                decay_trans_mat[ee, ee] = weight_decay_bias + centri_strength * 0.1\n",
    "                for p in sc:\n",
    "                    decay_trans_mat[ee, p] += -centri_strength * 0.1 / len(clst)\n",
    "        vec_mat = torch.from_numpy(decay_trans_mat).cuda()\n",
    "        result[kernel_namedvalue_list[layer_idx].name.replace(KERNEL_KEYWORD, 'bn.weight')] = vec_mat\n",
    "        result[kernel_namedvalue_list[layer_idx].name.replace(KERNEL_KEYWORD, 'bn.bias')] = vec_mat\n",
    "        result[kernel_namedvalue_list[layer_idx].name.replace(KERNEL_KEYWORD, 'conv.bias')] = vec_mat\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "param_name_to_decay_matrix = generate_decay_matrix_for_kernel_and_vecs(deps=deps,\n",
    "                                                                            layer_idx_to_clusters=layer_idx_to_clusters,\n",
    "                                                                            kernel_namedvalue_list=kernel_namedvalue_list,\n",
    "                                                                            weight_decay=weight_decay,\n",
    "                                                                            weight_decay_bias=weight_decay_bias,\n",
    "                                                                            centri_strength=centri_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['features.0.weight', 'features.2.weight', 'features.5.weight', 'features.7.weight', 'features.10.weight', 'features.12.weight', 'features.14.weight', 'features.17.weight', 'features.19.weight', 'features.21.weight', 'features.24.weight', 'features.26.weight', 'features.28.weight', 'features.0.bn.weight', 'features.0.bn.bias', 'features.0.conv.bias', 'features.2.bn.weight', 'features.2.bn.bias', 'features.2.conv.bias', 'features.5.bn.weight', 'features.5.bn.bias', 'features.5.conv.bias', 'features.7.bn.weight', 'features.7.bn.bias', 'features.7.conv.bias', 'features.10.bn.weight', 'features.10.bn.bias', 'features.10.conv.bias', 'features.12.bn.weight', 'features.12.bn.bias', 'features.12.conv.bias', 'features.14.bn.weight', 'features.14.bn.bias', 'features.14.conv.bias', 'features.17.bn.weight', 'features.17.bn.bias', 'features.17.conv.bias', 'features.19.bn.weight', 'features.19.bn.bias', 'features.19.conv.bias', 'features.21.bn.weight', 'features.21.bn.bias', 'features.21.conv.bias', 'features.24.bn.weight', 'features.24.bn.bias', 'features.24.conv.bias', 'features.26.bn.weight', 'features.26.bn.bias', 'features.26.conv.bias', 'features.28.bn.weight', 'features.28.bn.bias', 'features.28.conv.bias'])\n",
      "dict_keys(['features.0.weight', 'features.2.weight', 'features.5.weight', 'features.7.weight', 'features.10.weight', 'features.12.weight', 'features.14.weight', 'features.17.weight', 'features.19.weight', 'features.21.weight', 'features.24.weight', 'features.26.weight', 'features.28.weight', 'features.24.conv.bias', 'features.24.bn.weight', 'features.24.bn.bias', 'features.21.conv.bias', 'features.21.bn.weight', 'features.21.bn.bias', 'features.0.conv.bias', 'features.0.bn.weight', 'features.0.bn.bias', 'features.12.conv.bias', 'features.12.bn.weight', 'features.12.bn.bias', 'features.19.conv.bias', 'features.19.bn.weight', 'features.19.bn.bias', 'features.2.conv.bias', 'features.2.bn.weight', 'features.2.bn.bias', 'features.7.conv.bias', 'features.7.bn.weight', 'features.7.bn.bias', 'features.10.conv.bias', 'features.10.bn.weight', 'features.10.bn.bias', 'features.26.conv.bias', 'features.26.bn.weight', 'features.26.bn.bias', 'features.14.conv.bias', 'features.14.bn.weight', 'features.14.bn.bias', 'features.17.conv.bias', 'features.17.bn.weight', 'features.17.bn.bias', 'features.5.conv.bias', 'features.5.bn.weight', 'features.5.bn.bias', 'features.28.conv.bias', 'features.28.bn.weight', 'features.28.bn.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(param_name_to_decay_matrix.keys())\n",
    "print(param_name_to_merge_matrix.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上面这些步骤的操作，是为了拿到要聚类的通道的 index\n",
    "## 然后按照论文中的公式在更新对于的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(net, data, label, optimizer, criterion, param_name_to_merge_matrix, param_name_to_decay_matrix):\n",
    "    pred = net(data)\n",
    "    loss = criterion(pred, label)\n",
    "    loss.backward()\n",
    "    # 上面是正常的计算 loss, 和反向传播\n",
    "\n",
    "    #TODO note: C-SGD works here\n",
    "    for name, param in net.named_parameters():\n",
    "        name = name.replace('module.', '')\n",
    "        if name in param_name_to_merge_matrix:\n",
    "            p_dim = param.dim()\n",
    "            p_size = param.size()\n",
    "            # 获取原本梯度参数\n",
    "            if p_dim == 4:\n",
    "                param_mat = param.reshape(p_size[0], -1)\n",
    "                g_mat = param.grad.reshape(p_size[0], -1)\n",
    "            elif p_dim == 1:\n",
    "                param_mat = param.reshape(p_size[0], 1)\n",
    "                g_mat = param.grad.reshape(p_size[0], 1)\n",
    "            else:\n",
    "                assert p_dim == 2\n",
    "                param_mat = param\n",
    "                g_mat = param.grad\n",
    "            # 上面是获取当前的梯度，reshape 成 g_mat\n",
    "            # 下面是将 g_mat 按照文章中的公式，进行矩阵相乘和相加。\n",
    "            csgd_gradient = param_name_to_merge_matrix[name].matmul(g_mat) + param_name_to_decay_matrix[name].matmul(param_mat)\n",
    "            # 将计算的结果更新到参数梯度中。\n",
    "            param.grad.copy_(csgd_gradient.reshape(p_size))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d95e5ead8567333cdb1adc0fcf9b673d14119729d919656a92ca8568d5656d27"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('mmlab': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
